<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-19179423-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-19179423-2');
</script>
  <title>Neural Tagger Implementations</title>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">

  <style type="text/css">
body {
    padding: 0px;
    margin: 0px;
}
h1 {
    margin: 0px;
    text-align: center;
    padding: 10px;
}
div.buttons {
    width: 100%;
    display: flex;
    justify-content: center;
}
div.header-outer {
    background: #f3f3f3;
    display: flex;
    flex-direction: column;
    align-items: center;
    line-height: 130%;
    margin: 0px;
    padding: 10px;
    font-size: large;
}
div.header {
    max-width: 1000px;
}
button {
    cursor: pointer;
    margin: 5px;
    border: none;
    padding: 15px 32px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    outline: none;
}
#dybutton {
    background-color: #f8fad2;
    border: 3px solid #f8fad2;
}
#tfbutton {
    background-color: #d0eaf0;
    border: 3px solid #d0eaf0;
}
#ptbutton { 
    background-color: #fbe1d3;
    border: 3px solid #fbe1d3;
}
div.disqus {
    max-width: 1000px;
    margin: auto;
}

div.main {
    text-align: center;
    padding-top: 10px;
}
div.outer {
    white-space: nowrap;
    display: none;
}
span.description {
    text-align: left;
    vertical-align: top;
    font-size: large;
    line-height: 112%;
    width: 400px;
    white-space: normal;
    display: none;
}
span.description.dynet {
    background-color: #f8fad2;
    display: none;
}
span.description.pytorch {
    background-color: #fbe1d3;
    display: none;
}
span.description.tensorflow {
    background-color: #d0eaf0;
    display: none;
}
span.paragraph-start {
    font-weight: bold;
}
code {
    text-align: left;
    vertical-align: top;
    width: 100ch;
    display: none;
    padding-left: 5px;
    padding-right: 5px;
}
code.pytorch-line {
    display: inline-block;
    background: #fbe1d3;
    margin-top: 8px;
    max-height: 3px;
}
code.tensorflow-line {
    display: inline-block;
    background: #d0eaf0;
    margin-top: 8px;
    max-height: 3px;
}
pre { }
td.linenos { background-color: #f0f0f0; padding-right: 10px; }
span.lineno { background-color: #f0f0f0; padding: 0 5px 0 5px; }
pre { line-height: 125%; font-family: Menlo, "Courier New", Courier, monospace; margin: 0 }
body .bp { color: #0a5ac2 } /* Name.Builtin.Pseudo */
body .c1 { color: #6a727c } /* Comment.Single */
body .fm { color: #0a5ac2 } /* Name.Function.Magic */
body .k { color: #d63e4d } /* Keyword */
body .kn { color: #d63e4d; } /* Keyword.Namespace */
body .mf { color: #0a5ac2 } /* Literal.Number.Float */
body .mi { color: #0a5ac2 } /* Literal.Number.Integer */
body .n { color: #000000 } /* Variable */
body .nb { color: #0a5ac2 } /* Name.Builtin */
body .nc { color: #6f40bf } /* Name.Class */
body .nf { color: #6f40bf } /* Name.Function */
body .nn { color: #000000 } /* Name.Namespace */
body .o { color: #d63e4d } /* Operator */
body .ow { color: #d63e4d } /* Operator.Word */
body .p { color: #000000 } /* Parentheses */
body .s1 { color: #052e60 } /* Literal.String.Single */
body .s2 { color: #052e60 } /* Literal.String.Double */
body .sd { color: #052e60 } /* Literal.String.Doc */
body .vm { color: #0a5ac2 } 

body .c { color: #999988 } /* Comment */
body .cm { color: #6a727c } /* Comment.Multiline */
body .cp { color: #6a727c } /* Comment.Preproc */
body .cs { color: #6a727c } /* Comment.Special */
body .err { color: #a61717 } /* Error */
body .gd { color: #000000 } /* Generic.Deleted */
body .ge { color: #000000 } /* Generic.Emph */
body .gh { color: #999999 } /* Generic.Heading */
body .gi { color: #000000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #555555 } /* Generic.Prompt */
body .gr { color: #aa0000 } /* Generic.Error */
body .gs { } /* Generic.Strong */
body .gt { color: #aa0000 } /* Generic.Traceback */
body .gu { color: #aaaaaa } /* Generic.Subheading */
body .hll { }
body .kc { color: #000000; } /* Keyword.Constant */
body .kd { color: #000000; } /* Keyword.Declaration */
body .kp { color: #000000; } /* Keyword.Pseudo */
body .kr { color: #000000; } /* Keyword.Reserved */
body .kt { color: #445588; } /* Keyword.Type */
body .m { color: #009999 } /* Literal.Number */
body .no { color: #008080 } /* Name.Constant */
body .s { color: #d01040 } /* Literal.String */
body .na { color: #008080 } /* Name.Attribute */
body .nd { color: #3c5d5d } /* Name.Decorator */
body .ni { color: #800080 } /* Name.Entity */
body .ne { color: #990000 } /* Name.Exception */
body .nl { color: #990000 } /* Name.Label */
body .nt { color: #000080 } /* Name.Tag */
body .nv { color: #008080 } /* Name.Variable */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mh { color: #0a5ac2 } /* Literal.Number.Hex */
body .mo { color: #0a5ac2 } /* Literal.Number.Oct */
body .sb { color: #052e60 } /* Literal.String.Backtick */
body .sc { color: #052e60 } /* Literal.String.Char */
body .se { color: #052e60 } /* Literal.String.Escape */
body .sh { color: #052e60 } /* Literal.String.Heredoc */
body .si { color: #052e60 } /* Literal.String.Interpol */
body .sx { color: #052e60 } /* Literal.String.Other */
body .sr { color: #052e60 } /* Literal.String.Regex */
body .ss { color: #052e60 } /* Literal.String.Symbol */
body .vc { color: #008080 } /* Name.Variable.Class */
body .vg { color: #008080 } /* Name.Variable.Global */
body .vi { color: #008080 } /* Name.Variable.Instance */
body .il { color: #009999 } /* Literal.Number.Integer.Long */
</style>

</head>

<body>
<div class="header-outer">
<h1>Implementing a neural Part-of-Speech tagger</h1>
<h3>by Jonathan K. Kummerfeld <a href="http://www.jkk.name/">[site]</a></h3>
<div class=header>
<p>
DyNet, PyTorch and Tensorflow are complex frameworks with different ways of approaching neural network implementation and variations in default behaviour.
This page is intended to show how to implement the same non-trivial model in all three.
The design of the page is motivated by my own preference for a complete program with annotations, rather than the more common tutorial style of introducing code piecemeal in between discussion.
The design of the code is also geared towards providing a complete picture of how things fit together.
For a non-tutorial version of this code it would be better to use abstraction to improve flexibility, but that would have complicated the flow here.
</p>
<p>
<span class="paragraph-start">Model:</span>
The three implementations below all define a part-of-speech tagger with word embeddings initialised using GloVe, fed into a one-layer bidirectional LSTM, followed by a matrix multiplication to produce scores for tags.
They all score ~97.2% on the development set of the Penn Treebank.
The specific hyperparameter choices follows <a href="https://arxiv.org/abs/1806.04470">Yang, Liang, and Zhang (CoLing 2018)</a> and matches their performance for the setting without a CRF layer or character-based word embeddings.
The <a href="https://github.com/jkkummerfeld/neural-tagger-tutorial">repository</a> for this page provides the code in runnable form.
The only dependencies are the respective frameworks (DyNet <a href="https://github.com/clab/dynet/releases/tag/2.0.3">2.0.3</a>, PyTorch <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.1">0.4.1</a> and Tensorflow <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.9.0">1.9.0</a>).
</p>
<p>
<span class="paragraph-start">Website usage:</span> Use the buttons to show one or more implementations and their associated comments (note, depending on your screen size you may need to scroll to see all the code).
Matching or closely related content is aligned.
Framework-specific comments are highlighted in a colour that matches their button and a line is used to make the link from the comment to the code clear.
</p>
<p>
Making this helped me understand all three frameworks better. Hopefully you will find it informative too!
</p>

<div class="buttons">
<button class="button" id="dybutton" onmouseover="" onclick="toggleDyNet()">Show/Hide DyNet</button>
<button class="button" id="ptbutton" onmouseover="" onclick="togglePyTorch()">Show/Hide PyTorch</button>
<button class="button" id="tfbutton" onmouseover="" onclick="toggleTensorflow()">Show/Hide Tensorflow</button>
</div>
</div>

</div>


<div class="main">
<div class="outer shared-content">
<span class="description shared-content">We use argparse for processing command line arguments, random for shuffling our data, sys for flushing output, and numpy for handling vectors of data.<br /><br /></span><code class="dynet"><pre><span></span><span class="c1"># DyNet Implementation</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

</pre></code><code class="pytorch"><pre><span></span><span class="c1"># PyTorch Implementation</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

</pre></code><code class="tensorflow"><pre><span></span><span class="c1"># Tensorflow Implementation</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">Typically, we would make many of these constants command line arguments and tune using the development set. For simplicity, I have fixed their values here to match Jiang, Liang and Zhang (CoLing 2018).<br /><br /></span><code class="dynet"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;__PAD__&quot;</span>
<span class="n">UNK</span> <span class="o">=</span> <span class="s2">&quot;__UNK__&quot;</span>
<span class="n">DIM_EMBEDDING</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">LSTM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.015</span> 
<span class="n">LEARNING_DECAY_RATE</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">KEEP_PROB</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">GLOVE</span> <span class="o">=</span> <span class="s2">&quot;../data/glove.6B.100d.txt&quot;</span> 
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">1e-8</span>

</pre></code><code class="pytorch"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;__PAD__&quot;</span>
<span class="n">UNK</span> <span class="o">=</span> <span class="s2">&quot;__UNK__&quot;</span>
<span class="n">DIM_EMBEDDING</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">LSTM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.015</span> 
<span class="n">LEARNING_DECAY_RATE</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">KEEP_PROB</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">GLOVE</span> <span class="o">=</span> <span class="s2">&quot;../data/glove.6B.100d.txt&quot;</span> 
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">1e-8</span>

</pre></code><code class="tensorflow"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;__PAD__&quot;</span>
<span class="n">UNK</span> <span class="o">=</span> <span class="s2">&quot;__UNK__&quot;</span>
<span class="n">DIM_EMBEDDING</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">LSTM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.015</span> 
<span class="n">LEARNING_DECAY_RATE</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">KEEP_PROB</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">GLOVE</span> <span class="o">=</span> <span class="s2">&quot;../data/glove.6B.100d.txt&quot;</span> 
<span class="c1"># WEIGHT_DECAY = 1e-8 Not used, see note at the bottom of the page</span>

</pre></code></div>
<div class="outer dynet">
<span class="description dynet">Dynet library imports. The first allows us to configure DyNet from within code rather than on the command line: mem is the amount of system memory initially allocated (DyNet has its own memory management), autobatch toggles automatic parallelisation of computations, weight_decay rescales weights by (1 - decay) after every update, random_seed sets the seed for random number generation.<br /><br /></span><code class="dynet"><pre><span></span><span class="kn">import</span> <span class="nn">dynet_config</span>
<span class="n">dynet_config</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">mem</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">autobatch</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># dynet_config.set_gpu() for when we want to run with GPUs</span>
<span class="kn">import</span> <span class="nn">dynet</span> <span class="kn">as</span> <span class="nn">dy</span>

</pre></code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">PyTorch library import.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

</pre></code><code class="tensorflow">&nbsp;</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Tensorflow library import.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet"><div class="source"><pre><span></span><span class="c1"># Data reading</span>
<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span><span class="c1"># Data reading</span>
<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span><span class="c1"># Data reading</span>
<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">We are expecting a minor variation on the raw Penn Treebank data, with one line per sentence, tokens separated by spaces, and the tag for each token placed next to its word (the | works as a separator as it does not appear as a token).<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="sd">&quot;&quot;&quot;Example input:</span>
<span class="sd">    Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_src</span><span class="p">:</span>
            <span class="n">t_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;|&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">content</span>

<span class="k">def</span> <span class="nf">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">chars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="sd">&quot;&quot;&quot;Example input:</span>
<span class="sd">    Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_src</span><span class="p">:</span>
            <span class="n">t_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;|&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">content</span>

<span class="k">def</span> <span class="nf">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">chars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="sd">&quot;&quot;&quot;Example input:</span>
<span class="sd">    Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_src</span><span class="p">:</span>
            <span class="n">t_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;|&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">content</span>

<span class="k">def</span> <span class="nf">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">chars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">Reduce sparsity by replacing all digits with 0.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">For the purpose of this example we only have arguments for locations of the data.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;POS tagger.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;dev_data&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">train</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dev_data</span><span class="p">)</span>

</pre></code><code class="pytorch"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;POS tagger.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;dev_data&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">train</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dev_data</span><span class="p">)</span>

</pre></code><code class="tensorflow"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;POS tagger.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;dev_data&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">train</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dev_data</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">These indices map from strings to integers, which we apply to the input for our model. UNK is added to our mapping so that there is a vector we can use when we encounter unknown words. The special PAD symbol is used in PyTorch and Tensorflow as part of shaping the data in a batch to be a consistent size. It is not needed for DyNet, but kept for consistency.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="c1"># Make indices</span>
    <span class="n">id_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">UNK</span><span class="p">]</span>
    <span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">UNK</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="n">id_to_tag</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span>
    <span class="n">tag_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="c1"># Make indices</span>
    <span class="n">id_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">UNK</span><span class="p">]</span>
    <span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">UNK</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="n">id_to_tag</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span>
    <span class="n">tag_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="c1"># Make indices</span>
    <span class="n">id_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">UNK</span><span class="p">]</span>
    <span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">UNK</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="n">id_to_tag</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span>
    <span class="n">tag_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">The '+ dev' may seem like an error, but is done here for convenience. It means in the next section we will retain the GloVe embeddings that appear in dev but not train. They won't be updated during training, so it does not mean we are getting information we shouldn't. In practise I would simply keep all the GloVe embeddings to avoid any potential incorrect use of the evaluation data.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">train</span> <span class="o">+</span> <span class="n">dev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_to_id</span><span class="p">:</span>
                <span class="n">token_to_id</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
                <span class="n">id_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tag_to_id</span><span class="p">:</span>
                <span class="n">tag_to_id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>
                <span class="n">id_to_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="n">NWORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
    <span class="n">NTAGS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>

    <span class="c1"># Load pre-trained GloVe vectors</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">train</span> <span class="o">+</span> <span class="n">dev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_to_id</span><span class="p">:</span>
                <span class="n">token_to_id</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
                <span class="n">id_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tag_to_id</span><span class="p">:</span>
                <span class="n">tag_to_id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>
                <span class="n">id_to_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="n">NWORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
    <span class="n">NTAGS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>

    <span class="c1"># Load pre-trained GloVe vectors</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">train</span> <span class="o">+</span> <span class="n">dev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_to_id</span><span class="p">:</span>
                <span class="n">token_to_id</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
                <span class="n">id_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tag_to_id</span><span class="p">:</span>
                <span class="n">tag_to_id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>
                <span class="n">id_to_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="n">NWORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
    <span class="n">NTAGS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>

    <span class="c1"># Load pre-trained GloVe vectors</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">I am assuming these are 100-dimensional GloVe embeddings in their standard format.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">pretrained</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">GLOVE</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">pretrained</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">GLOVE</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="n">pretrained</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">GLOVE</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">We need the word vectors as a list to initialise the embeddings. Each entry in the list corresponds to the token with that index.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">pretrained_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">/</span> <span class="n">DIM_EMBEDDING</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">id_to_token</span><span class="p">:</span>
        <span class="c1"># apply lower() because all GloVe vectors are for lowercase words</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">pretrained</span><span class="p">:</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]))</span>
        <span class="k">else</span><span class="p">:</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">pretrained_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">/</span> <span class="n">DIM_EMBEDDING</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">id_to_token</span><span class="p">:</span>
        <span class="c1"># apply lower() because all GloVe vectors are for lowercase words</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">pretrained</span><span class="p">:</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]))</span>
        <span class="k">else</span><span class="p">:</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="n">pretrained_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">/</span> <span class="n">DIM_EMBEDDING</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">id_to_token</span><span class="p">:</span>
        <span class="c1"># apply lower() because all GloVe vectors are for lowercase words</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">pretrained</span><span class="p">:</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]))</span>
        <span class="k">else</span><span class="p">:</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">For words that do not appear in GloVe we generate a random vector (note, the choice of scale here is important and we follow Jiang, Liang and Zhang (CoLing 2018).<br /><br /></span><code class="dynet"><pre><span></span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="n">DIM_EMBEDDING</span><span class="p">])</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_vector</span><span class="p">)</span>

</pre></code><code class="pytorch"><pre><span></span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="n">DIM_EMBEDDING</span><span class="p">])</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_vector</span><span class="p">)</span>

</pre></code><code class="tensorflow"><pre><span></span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="n">DIM_EMBEDDING</span><span class="p">])</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_vector</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">The most significant difference between the frameworks is how the model parameters and their execution is defined. In DyNet we define parameters here and then define computation as needed. In PyTorch we use a class with the parameters defined in the constructor and the computation defined in the forward() method. In Tensorflow we define both parameters and computation here.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="c1"># Model creation</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="c1"># Model creation</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="c1"># Model creation</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">model</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">ParameterCollection</span><span class="p">()</span>
    <span class="c1"># Create word embeddings and initialise</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">Lookup parameters are a matrix that supports efficient sparse lookup.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">pEmbedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">add_lookup_parameters</span><span class="p">((</span><span class="n">NWORDS</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">))</span>
    <span class="n">pEmbedding</span><span class="o">.</span><span class="n">init_from_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained_list</span><span class="p">))</span>
    <span class="c1"># Create LSTM parameters</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">Objects that create LSTM cells and the necessary parameters.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">stdv</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span><span class="p">)</span> 
    <span class="n">f_lstm</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">VanillaLSTMBuilder</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
            <span class="n">forget_bias</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stdv</span><span class="p">)</span>
    <span class="n">b_lstm</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">VanillaLSTMBuilder</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
            <span class="n">forget_bias</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stdv</span><span class="p">)</span>
    <span class="c1"># Create output layer</span>
    <span class="n">pOutput</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">add_parameters</span><span class="p">((</span><span class="n">NTAGS</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">))</span>
    
    <span class="c1"># Set recurrent dropout values (not used in this case)</span>
    <span class="n">f_lstm</span><span class="o">.</span><span class="n">set_dropouts</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">set_dropouts</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="c1"># Initialise LSTM parameters</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">To match PyTorch, we initialise the parameters with an unconventional approach.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">f_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">]))</span>
    <span class="n">f_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>
    <span class="n">f_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">]))</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>

</pre></code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">The trainer object is used to update the model.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="c1"># Create the trainer</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">SimpleSGDTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">DyNet clips gradients by default, which we disable here (this can have a big impact on performance).<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">trainer</span><span class="o">.</span><span class="n">set_clip_threshold</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

</pre></code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">model</span> <span class="o">=</span> <span class="n">TaggerModel</span><span class="p">(</span><span class="n">NWORDS</span><span class="p">,</span> <span class="n">NTAGS</span><span class="p">,</span> <span class="n">pretrained_list</span><span class="p">,</span> <span class="n">id_to_token</span><span class="p">)</span>
    <span class="c1"># Create optimizer and configure the learning rate</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">)</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">The learning rate for each epoch is set by multiplying the initial rate by the factor produced by this function.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><pre><span></span>    <span class="n">rescale_lr</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">LEARNING_DECAY_RATE</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="n">lr_lambda</span><span class="o">=</span><span class="n">rescale_lr</span><span class="p">)</span>

</pre></code><code class="tensorflow">&nbsp;</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">
<br />
This line creates a new graph and makes it the default graph for operations to be registered to. It is not necessary here because we only have one graph, but is considered good practise (more discussion on <a href="https://stackoverflow.com/questions/39614938/why-do-we-need-tensorflow-tf-graph">Stackoverflow</a>.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Placeholders are inputs/values that will be fed into the network each time it is run. We define their type, name, and shape (constant, 1D vector, 2D vector, etc). This includes what we normally think of as inputs (e.g. the tokens) as well as parameters we want to change at run time (e.g. the learning rate).<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1"># Define inputs</span>
        <span class="n">e_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>
        <span class="n">e_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lengths&#39;</span><span class="p">)</span>
        <span class="n">e_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mask&#39;</span><span class="p">)</span>
        <span class="n">e_gold_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gold_output&#39;</span><span class="p">)</span>
        <span class="n">e_keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">)</span>
        <span class="n">e_learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">)</span>

        <span class="c1"># Define word embedding</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">The embedding matrix is a variable (so they can shift in training), initialized with the vectors defined above.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">glove_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained_list</span><span class="p">))</span>
        <span class="n">e_embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;embedding&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">NWORDS</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">],</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">glove_init</span><span class="p">)</span>
        <span class="n">e_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">e_embedding</span><span class="p">,</span> <span class="n">e_input</span><span class="p">)</span>

        <span class="c1"># Define LSTM cells</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We create an LSTM cell, then wrap it in a class that applies dropout to the input and output.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_cell_f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span><span class="p">)</span>
        <span class="n">e_cell_f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">e_cell_f</span><span class="p">,</span>
                <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">)</span>
        <span class="c1"># Recurrent dropout options</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We are not using recurrent dropout, but it is a common enough feature of networks that it's good to see how it is done.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1">#        variational_recurrent=True, dtype=tf.float32,</span>
        <span class="c1">#        input_size=DIM_EMBEDDING)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Similarly, multi-layer networks are a common use case. In Tensorflow, we would wrap a list of cells with a MultiRNNCell.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1"># Multi-layer cell creation</span>
        <span class="c1"># e_cell_f = tf.contrib.rnn.MultiRNNCell([e_cell_f])</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We are making a bidirectional network, so we need another cell for the reverse direction.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_cell_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span><span class="p">)</span>
        <span class="n">e_cell_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">e_cell_b</span><span class="p">,</span>
                <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">To use the cells we create a dynamic RNN. The 'dynamic' aspect means we can feed in the lengths of input sequences not counting padding and it will stop early.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_initial_state_f</span> <span class="o">=</span> <span class="n">e_cell_f</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">e_initial_state_b</span> <span class="o">=</span> <span class="n">e_cell_f</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">e_lstm_outputs</span><span class="p">,</span> <span class="n">e_final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bidirectional_dynamic_rnn</span><span class="p">(</span>
                <span class="n">cell_fw</span><span class="o">=</span><span class="n">e_cell_f</span><span class="p">,</span> <span class="n">cell_bw</span><span class="o">=</span><span class="n">e_cell_b</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">e_embed</span><span class="p">,</span>
                <span class="n">initial_state_fw</span><span class="o">=</span><span class="n">e_initial_state_f</span><span class="p">,</span>
                <span class="n">initial_state_bw</span><span class="o">=</span><span class="n">e_initial_state_b</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="n">e_lengths</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">e_lstm_outputs_merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">e_lstm_outputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Define output layer</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Matrix multiply to get scores for each class.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">e_lstm_outputs_merged</span><span class="p">,</span>
                <span class="n">NTAGS</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="c1"># Define loss and update</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Cross-entropy loss. The reduction flag is crucial (the default is to average over the sequence). The weights flag accounts for padding that makes all of the sequences the same length.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">e_gold_output</span><span class="p">,</span>
                <span class="n">e_predictions</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">e_mask</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="n">e_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">e_learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">e_loss</span><span class="p">)</span>
        <span class="c1"># Update with gradient clipping</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">If we wanted to do gradient clipping we would need to do the update in a few steps, first calculating the gradient, then modifying it before applying it.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1"># e_optimiser = tf.train.GradientDescentOptimizer(LEARNING_RATE)</span>
        <span class="c1"># e_gradients = e_optimiser.compute_gradients(e_loss)</span>
        <span class="c1"># e_clipped_gradients = [(tf.clip_by_value(grad, -5., 5.), var)</span>
        <span class="c1">#         for grad, var in e_gradients]</span>
        <span class="c1"># e_train = e_optimiser.apply_gradients(e_gradients)</span>

        <span class="c1"># Define output</span>
        <span class="n">e_auto_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">e_predictions</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># Do training</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Configure the system environment. By default Tensorflow uses all available GPUs and RAM. These lines limit the number of GPUs used and the amount of RAM. To limit which GPUs are used, set the environment variable CUDA_VISIBLE_DEVICES (e.g. "export CUDA_VISIBLE_DEVICES=0,1").<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
            <span class="n">device_count</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;GPU&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
            <span class="n">gpu_options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span><span class="n">per_process_gpu_memory_fraction</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">A session runs the graph. We use a 'with' block to ensure it is closed, which frees various resources.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Run executes operations, in this case initializing the variables.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><pre><span></span>            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">To make the code match across the three versions, we group together some framework specific values needed when doing a pass over the data.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">expressions</span> <span class="o">=</span> <span class="p">(</span><span class="n">pEmbedding</span><span class="p">,</span> <span class="n">pOutput</span><span class="p">,</span> <span class="n">f_lstm</span><span class="p">,</span> <span class="n">b_lstm</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">expressions</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>            <span class="n">expressions</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">e_auto_output</span><span class="p">,</span> <span class="n">e_gold_output</span><span class="p">,</span> <span class="n">e_input</span><span class="p">,</span> <span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">e_lengths</span><span class="p">,</span>
                <span class="n">e_loss</span><span class="p">,</span> <span class="n">e_train</span><span class="p">,</span> <span class="n">e_mask</span><span class="p">,</span> <span class="n">e_learning_rate</span><span class="p">,</span> <span class="n">sess</span>
            <span class="p">]</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">Main training loop, in which we shuffle the data, set the learning rate, do one complete pass over the training data, then evaluate on the development data.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

</pre></code><code class="pytorch"><pre><span></span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

</pre></code><code class="tensorflow"><pre><span></span>            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
                <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet"><pre><span></span>        <span class="c1"># Update learning rate</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="n">LEARNING_DECAY_RATE</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">)</span>

</pre></code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="c1"># Update learning rate</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">First call to rescale_lr is with a 0, which is why this must be done before the pass over the data.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><pre><span></span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

</pre></code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Training mode (and evaluation mode below) do things like enable dropout components.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet">&nbsp;</code><code class="pytorch">&nbsp;</code><code class="tensorflow"><pre><span></span>                <span class="c1"># Determine the current learning rate</span>
                <span class="n">current_lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="n">LEARNING_DECAY_RATE</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">Training pass.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span>
                <span class="n">current_lr</span><span class="p">)</span>
</pre></div>
</code><code class="pytorch"><pre><span></span>        <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                <span class="bp">True</span><span class="p">)</span>

</pre></code><code class="tensorflow"><div class="source"><pre><span></span>                <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                        <span class="bp">True</span><span class="p">,</span> <span class="n">current_lr</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">Dev pass.<br /><br /></span><code class="dynet"><pre><span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">dacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} loss {} t-acc {} d-acc {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span><span class="p">,</span> <span class="n">dacc</span><span class="p">))</span>

</pre></code><code class="pytorch"><pre><span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">dacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} loss {} t-acc {} d-acc {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
            <span class="n">tacc</span><span class="p">,</span> <span class="n">dacc</span><span class="p">))</span>

</pre></code><code class="tensorflow"><pre><span></span>                <span class="n">_</span><span class="p">,</span> <span class="n">dacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                        <span class="bp">False</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} loss {} t-acc {} d-acc {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span><span class="p">,</span>
                    <span class="n">dacc</span><span class="p">))</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">The syntax varies, but in all three cases either saving or loading the parameters of a model must be done after the model is defined.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="c1"># Save model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;tagger.dy.model&quot;</span><span class="p">)</span>

    <span class="c1"># Load model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">populate</span><span class="p">(</span><span class="s2">&quot;tagger.dy.model&quot;</span><span class="p">)</span>

    <span class="c1"># Evaluation pass.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>

</pre></code><code class="pytorch"><pre><span></span>    <span class="c1"># Save model</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;tagger.pt.model&quot;</span><span class="p">)</span>

    <span class="c1"># Load model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tagger.pt.model&#39;</span><span class="p">))</span>

    <span class="c1"># Evaluation pass.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>

</pre></code><code class="tensorflow"><pre><span></span>            <span class="c1"># Save model</span>
            <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
            <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./tagger.tf.model&quot;</span><span class="p">)</span>

            <span class="c1"># Load model</span>
            <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./tagger.tf.model&quot;</span><span class="p">)</span>

            <span class="c1"># Evaluation pass.</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                    <span class="bp">False</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>

</pre></code></div>
<div class="outer pytorch">
<span class="description pytorch">Neural network definition code. In PyTorch networks are defined using classes that extend Module.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span><span class="k">class</span> <span class="nc">TaggerModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">In the constructor we define objects that will do each of the computations.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nwords</span><span class="p">,</span> <span class="n">ntags</span><span class="p">,</span> <span class="n">pretrained_list</span><span class="p">,</span> <span class="n">id_to_token</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Create word embeddings</span>
        <span class="n">pretrained_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">pretrained_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">pretrained_tensor</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1"># Create input dropout parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span>
        <span class="c1"># Create LSTM parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">DIM_EMBEDDING</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># Create output dropout parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_output_dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span>
        <span class="c1"># Create final matrix multiply parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_to_tag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ntags</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">cur_batch_size</span><span class="p">):</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">sentences</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Look up word vectors</span>
        <span class="n">word_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
        <span class="c1"># Apply dropout</span>
        <span class="n">dropped_word_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">)</span>
        <span class="c1"># Run the LSTM over the input, reshaping data for efficiency</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Assuming the data is ordered longest to shortest, this provides a view of the data that fits with how cuDNN works.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">packed_words</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span>
                <span class="n">dropped_word_vectors</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">The None argument is an optional initial hidden state (default is a zero vector). The ignored return value contains the hidden states.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">packed_words</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Reverse the view shift made for cuDNN. Specifying total_length is not necessary in general (it can be inferred), but is necessary for parallel processing. The ignored return value contains the length of each sequence.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">,</span>
                <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">total_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
        <span class="c1"># Apply dropout</span>
        <span class="n">lstm_out_dropped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_output_dropout</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
        <span class="c1"># Matrix multiply to get scores for each tag</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_to_tag</span><span class="p">(</span><span class="n">lstm_out_dropped</span><span class="p">)</span>

        <span class="c1"># Calculate loss and predictions</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">We reshape to [batch size * sequence length , ntags] for more efficient processing.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span> <span class="o">*</span> <span class="n">max_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">flat_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span> <span class="o">*</span> <span class="n">max_length</span><span class="p">)</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">The ignore index refers to outputs to not score, which we use to ignore padding. 'reduction' defines how to combine the losses at each point in the sequence. The default is elementwise_mean, which would not do what we want.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">loss_function</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">output_scores</span><span class="p">,</span> <span class="n">flat_labels</span><span class="p">)</span>
        <span class="n">predicted_tags</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output_scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Reshape to have dimensions [batch size , sequence length].<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><pre><span></span>        <span class="n">predicted_tags</span> <span class="o">=</span> <span class="n">predicted_tags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">predicted_tags</span>

</pre></code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">Inference (the same function for train and test).<br /><br /></span><code class="dynet"><div class="source"><pre><span></span><span class="k">def</span> <span class="nf">do_pass</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
    <span class="n">pEmbedding</span><span class="p">,</span> <span class="n">pOutput</span><span class="p">,</span> <span class="n">f_lstm</span><span class="p">,</span> <span class="n">b_lstm</span><span class="p">,</span> <span class="n">trainer</span> <span class="o">=</span> <span class="n">expressions</span>

    <span class="c1"># Loop over batches</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span><span class="k">def</span> <span class="nf">do_pass</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">expressions</span>

    <span class="c1"># Loop over batches</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span><span class="k">def</span> <span class="nf">do_pass</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="n">e_auto_output</span><span class="p">,</span> <span class="n">e_gold_output</span><span class="p">,</span> <span class="n">e_input</span><span class="p">,</span> <span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">e_lengths</span><span class="p">,</span> <span class="n">e_loss</span><span class="p">,</span> \
            <span class="n">e_train</span><span class="p">,</span> <span class="n">e_mask</span><span class="p">,</span> <span class="n">e_learning_rate</span><span class="p">,</span> <span class="n">session</span> <span class="o">=</span> <span class="n">expressions</span>

    <span class="c1"># Loop over batches</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">Form the batch and order it based on length (important for efficient processing in PyTorch).<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">BATCH_SIZE</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">BATCH_SIZE</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">BATCH_SIZE</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">Log partial results so we can conveniently check progress.<br /><br /></span><code class="dynet"><pre><span></span>        <span class="k">if</span> <span class="n">start</span> <span class="o">%</span> <span class="mi">4000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

</pre></code><code class="pytorch"><pre><span></span>        <span class="k">if</span> <span class="n">start</span> <span class="o">%</span> <span class="mi">4000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

</pre></code><code class="tensorflow"><pre><span></span>        <span class="k">if</span> <span class="n">start</span> <span class="o">%</span> <span class="mi">4000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

</pre></code></div>
<div class="outer dynet">
<span class="description dynet">Start a new computation graph for this batch.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="c1"># Process batch</span>
        <span class="n">dy</span><span class="o">.</span><span class="n">renew_cg</span><span class="p">()</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">For each example, we will construct an expression that gives the loss.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">loss_expressions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="c1"># Prepare inputs</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Prepare input arrays, using .long() to cast the type from Tensor to LongTensor.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
        <span class="n">input_array</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">output_array</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet">&nbsp;</code><code class="pytorch">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1"># Add empty sentences to fill the batch</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We add empty sentences because Tensorflow requires every batch to be the same size.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">+=</span> <span class="p">[([],</span> <span class="p">[])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))]</span>
        <span class="c1"># Prepare inputs</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We do this here for convenience and to have greater alignment between implementations, but in practise it would be best to do this once in pre-processing.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">input_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">max_length</span><span class="p">])</span>
        <span class="n">output_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">max_length</span><span class="p">])</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">max_length</span><span class="p">])</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content">Convert tokens and tags from strings to numbers using the indices.<br /><br /></span><code class="dynet"><pre><span></span>        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">simplify_token</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">tag_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag_to_id</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>

</pre></code><code class="pytorch"><pre><span></span>        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">simplify_token</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">tag_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag_to_id</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>

</pre></code><code class="tensorflow"><pre><span></span>        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">simplify_token</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">tag_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag_to_id</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>

</pre></code></div>
<div class="outer dynet">
<span class="description dynet">Now we define the computation to be performed with the model. Note that they are not applied yet, we are simply building the computation graph.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>            <span class="c1"># Look up word embeddings</span>
            <span class="n">wembs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dy</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">pEmbedding</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
            <span class="c1"># Apply dropout</span>
            <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
                <span class="n">wembs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dy</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wembs</span><span class="p">]</span>
            <span class="c1"># Feed words into the LSTM</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">Create an expression for two LSTMs and feed in the embeddings (reversed in one case).
<br />
We pull out the output vector from the cell state at each step.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>            <span class="n">f_init</span> <span class="o">=</span> <span class="n">f_lstm</span><span class="o">.</span><span class="n">initial_state</span><span class="p">()</span>
            <span class="n">f_lstm_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">output</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">f_init</span><span class="o">.</span><span class="n">add_inputs</span><span class="p">(</span><span class="n">wembs</span><span class="p">)]</span>
            <span class="n">rev_embs</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">wembs</span><span class="p">)</span>
            <span class="n">b_init</span> <span class="o">=</span> <span class="n">b_lstm</span><span class="o">.</span><span class="n">initial_state</span><span class="p">()</span>
            <span class="n">b_lstm_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">output</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">b_init</span><span class="o">.</span><span class="n">add_inputs</span><span class="p">(</span><span class="n">rev_embs</span><span class="p">)]</span>

            <span class="c1"># For each output, calculate the output and loss</span>
            <span class="n">pred_tags</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">f_lstm_output</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">b_lstm_output</span><span class="p">),</span> <span class="n">tag_ids</span><span class="p">):</span>
                <span class="c1"># Combine the outputs</span>
                <span class="n">combined</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">f</span><span class="p">,</span><span class="n">b</span><span class="p">])</span>
                <span class="c1"># Apply dropout</span>
                <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
                    <span class="n">combined</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span>
                <span class="c1"># Matrix multiply to get scores for each tag</span>
                <span class="n">r_t</span> <span class="o">=</span> <span class="n">pOutput</span> <span class="o">*</span> <span class="n">combined</span>
                <span class="c1"># Calculate cross-entropy loss</span>
                <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
                    <span class="n">err</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">pickneglogsoftmax</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">We are not actually evaluating the loss values here, instead we collect them together in a list. This enables DyNet's <a href="http://dynet.readthedocs.io/en/latest/tutorials_notebooks/Autobatching.html">autobatching</a>.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>                    <span class="n">loss_expressions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                <span class="c1"># Calculate the highest scoring tag</span>
</pre></div>
</code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet">This call to .npvalue() will lead to evaluation of the graph and so we don't actually get the benefits of autobatching. With some refactoring we could get the benefit back (simply keep the r_t expressions around and do this after the update), but that would have complicated this code.<br /><br /></span><code class="dynet"><pre><span></span>                <span class="n">chosen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">r_t</span><span class="o">.</span><span class="n">npvalue</span><span class="p">())</span>
                <span class="n">pred_tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chosen</span><span class="p">)</span>
            <span class="n">predicted</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_tags</span><span class="p">)</span>

        <span class="c1"># combine the losses for the batch, do an update, and record the loss</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">loss_for_batch</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">esum</span><span class="p">(</span><span class="n">loss_expressions</span><span class="p">)</span>
            <span class="n">loss_for_batch</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_for_batch</span><span class="o">.</span><span class="n">scalar_value</span><span class="p">()</span>

</pre></code><code class="pytorch">&nbsp;</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>            <span class="n">input_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
            <span class="n">output_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tags</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">tag_ids</span><span class="p">)</span>

        <span class="c1"># Construct computation</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Calling the model as a function will run its forward() function, which constructs the computations.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">batch_loss</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_array</span><span class="p">,</span> <span class="n">output_array</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span>
                <span class="n">cur_batch_size</span><span class="p">)</span>

        <span class="c1"># Run computations</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">To get the loss value we use .item().<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><div class="source"><pre><span></span>            <span class="n">loss</span> <span class="o">+=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</code><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch">Our output is an array (rather than a single value), so we use a different approach to get it into a usable form.<br /><br /></span><code class="dynet pytorch-line">&nbsp;</code><code class="pytorch"><pre><span></span>        <span class="n">predicted</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

</pre></code><code class="tensorflow">&nbsp;</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>            <span class="n">input_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token_ids</span>
            <span class="n">output_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tags</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tag_ids</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)])</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We can't change the computation graph to disable dropout when not training, so we just change the keep probability.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">cur_keep_prob</span> <span class="o">=</span> <span class="n">KEEP_PROB</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="mf">1.0</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">This dictionary contains values for all of the placeholders we defined.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">e_input</span><span class="p">:</span> <span class="n">input_array</span><span class="p">,</span>
                <span class="n">e_gold_output</span><span class="p">:</span> <span class="n">output_array</span><span class="p">,</span>
                <span class="n">e_mask</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>
                <span class="n">e_keep_prob</span><span class="p">:</span> <span class="n">cur_keep_prob</span><span class="p">,</span>
                <span class="n">e_lengths</span><span class="p">:</span> <span class="n">lengths</span><span class="p">,</span>
                <span class="n">e_learning_rate</span><span class="p">:</span> <span class="n">lr</span>
        <span class="p">}</span>

        <span class="c1"># Define the computations needed</span>
        <span class="n">todo</span> <span class="o">=</span> <span class="p">[</span><span class="n">e_auto_output</span><span class="p">]</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">If we are not training we do not need to compute a loss and we do not want to do the update.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e_loss</span><span class="p">)</span>
            <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e_train</span><span class="p">)</span>
        <span class="c1"># Run computations</span>
        <span class="n">outcomes</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">todo</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">)</span>
        <span class="c1"># Get outputs</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="n">outcomes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow">We do not request the e_train value because its work is done - it performed the update during its computation.<br /><br /></span><code class="dynet tensorflow-line">&nbsp;</code><code class="pytorch tensorflow-line">&nbsp;</code><code class="tensorflow"><pre><span></span>            <span class="n">loss</span> <span class="o">+=</span> <span class="n">outcomes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content">&nbsp;</span><code class="dynet"><div class="source"><pre><span></span>        <span class="c1"># Update the number of correct tags and total tags</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">),</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">gt</span><span class="p">,</span> <span class="n">at</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
                <span class="n">gt</span> <span class="o">=</span> <span class="n">tag_to_id</span><span class="p">[</span><span class="n">gt</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">gt</span> <span class="o">==</span> <span class="n">at</span><span class="p">:</span>
                    <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</code><code class="pytorch"><div class="source"><pre><span></span>        <span class="c1"># Update the number of correct tags and total tags</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">),</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">gt</span><span class="p">,</span> <span class="n">at</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
                <span class="n">gt</span> <span class="o">=</span> <span class="n">tag_to_id</span><span class="p">[</span><span class="n">gt</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">gt</span> <span class="o">==</span> <span class="n">at</span><span class="p">:</span>
                    <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</code><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1"># Update the number of correct tags and total tags</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">),</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">gt</span><span class="p">,</span> <span class="n">at</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
                <span class="n">gt</span> <span class="o">=</span> <span class="n">tag_to_id</span><span class="p">[</span><span class="n">gt</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">gt</span> <span class="o">==</span> <span class="n">at</span><span class="p">:</span>
                    <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</code></div>
<br /></div>


<script>
var dyShowing = false;
var tfShowing = false;
var ptShowing = false;
function whichShowing() {
    if (dyShowing && tfShowing && ptShowing) return "all";
    else if (dyShowing && ptShowing) return "-t";
    else if (dyShowing && tfShowing) return "-p";
    else if (ptShowing && tfShowing) return "-d";
    else if (dyShowing) return "d";
    else if (ptShowing) return "p";
    else if (tfShowing) return "t";
    else return "-";
}
function toggleItem(toEdit, showing) {
    if (
        (showing === "d" && toEdit.classList.contains("dynet")) ||
        (showing === "p" && toEdit.classList.contains("pytorch")) ||
        (showing === "t" && toEdit.classList.contains("tensorflow")) ||
        (showing === "-d" && toEdit.classList.contains("pytorch")) ||
        (showing === "-p" && toEdit.classList.contains("dynet")) ||
        (showing === "-t" && toEdit.classList.contains("dynet")) ||
        (showing === "-d" && toEdit.classList.contains("tensorflow")) ||
        (showing === "-p" && toEdit.classList.contains("tensorflow")) ||
        (showing === "-t" && toEdit.classList.contains("pytorch")) ||
        (showing != "-" && toEdit.classList.contains("shared-content")) ||
        showing === "all"
       ) {
        if (toEdit.classList.contains('outer')) {
            toEdit.style.display = "block";
        } else {
            toEdit.style.display = "inline-block";
        }
    } else {
        toEdit.style.display = "none";
    }
}
function toggleDyNet() {
    dyShowing = ! dyShowing;

    var dybutton = document.getElementById("dybutton");
    if (dyShowing) {
        dybutton.style.backgroundColor = "#f1f5a4";
        dybutton.style.border = "3px solid #777777";
    } else {
        dybutton.style.backgroundColor = "#f8fad2";
        dybutton.style.border = "3px solid #f8fad2";
    }

    toggleAll();
}
function togglePyTorch() {
    ptShowing = ! ptShowing;

    var ptbutton = document.getElementById("ptbutton");
    if (ptShowing) {
        ptbutton.style.backgroundColor = "#f7c1a4";
        ptbutton.style.border = "3px solid #777777";
    } else {
        ptbutton.style.backgroundColor = "#fbe1d3";
        ptbutton.style.border = "3px solid #fbe1d3";
    }

    toggleAll();
}
function toggleTensorflow() {
    tfShowing = ! tfShowing;

    var tfbutton = document.getElementById("tfbutton");
    if (tfShowing) {
        tfbutton.style.backgroundColor = "#a9d9e4";
        tfbutton.style.border = "3px solid #777777";
    } else {
        tfbutton.style.backgroundColor = "#d0eaf0";
        tfbutton.style.border = "3px solid #d0eaf0";
    }

    toggleAll();
}
function toggleAll() {
    showing = whichShowing();
    var dyitems = document.getElementsByClassName("dynet");
    for (var i = dyitems.length - 1; i >= 0; i--) {
        toggleItem(dyitems[i], showing);
    }
    var tfitems = document.getElementsByClassName("tensorflow");
    for (var i = tfitems.length - 1; i >= 0; i--) {
        toggleItem(tfitems[i], showing);
    }
    var ptitems = document.getElementsByClassName("pytorch");
    for (var i = ptitems.length - 1; i >= 0; i--) {
        toggleItem(ptitems[i], showing);
    }
    var allitems = document.getElementsByClassName("shared-content");
    for (var i = allitems.length - 1; i >= 0; i--) {
        toggleItem(allitems[i], showing);
    }
}
</script>

<div class="header-outer">
<div class="header">
<p>
This code was last updated in August 2018.
If one of the frameworks has changed in a way that should be reflected here, please let me know!
</p>
<p>
A few miscellaneous notes:
<ul>
    <li>PyTorch 0.4 does not support recurrent dropout directly. For an example of how to achieve it, see the LSTM and QRNN Language Model Toolkit's <a href="https://github.com/salesforce/awd-lstm-lm/blob/28683b20154fce8e5812aeb6403e35010348c3ea/weight_drop.py">WeightDrop class</a> and <a href="https://github.com/salesforce/awd-lstm-lm/blob/457a422eb46e970a6aad659ca815a04b3d074d6c/model.py#L22">how it is used</a>.</li>
    <li>Tensorflow 1.9 does not support weight decay directly, but <a href="https://github.com/tensorflow/tensorflow/pull/17438">this pull request</a> appears to add support and will be part of 1.10.</li>
</ul>
</p>
<p>
I developed this code and webpage with help from many people and resources. In particular:
<ul>
    <li> Feedback from <a href="http://proebsting.cs.arizona.edu/">Todd Proebsting</a>, <a href="http://www.it.usyd.edu.au/~judy/">Judy Kay</a>, <a href="http://www.it.usyd.edu.au/~bob/">Bob Kummerfeld</a>, and members of the <a href="http://web.eecs.umich.edu/~wlasecki/croma.html">CROMA Lab</a>.</li>
    <li> <a href="https://github.com/jiesutd/NCRFpp">NCRFpp</a>, the code associated with <a href="https://arxiv.org/abs/1806.04470">Yang, Liang, and Zhang (CoLing 2018)</a>, which was my starting point for PyTorch and my reference point when trying to check performance for the others.</li>
    <li> Guillaume Genthial's blog post about <a href="https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html">Sequence Tagging with Tensorflow</a>. </li>
    <li> The DyNet <a href="https://github.com/clab/dynet/blob/master/examples/tagger/bilstmtagger.py">example tagger</a>. </li>
</ul>
</p>
</div>
</div>

<div class="disqus">
<div id="disqus_thread"></div>
</div>
<script>
var disqus_config = function () {
    this.page.url = 'http://jkk.name/neural-tagger-tutorial/';
    this.page.identifier = '/neural-tagger-tutorial/';
    this.page.title = 'Neural Tagger Example';
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://www-jkk-name.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</body>
</html>
